{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (5.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (1.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from sentence_transformers) (5.0.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from sentence_transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from sentence_transformers) (1.15.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (4.67.3)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipywidgets) (8.38.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (4.67.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\sungpo\\anaconda3\\envs\\ml_gpu\\lib\\site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your API token from your Hugging Face Account. Make sure to save it in text file or notepad for future use.\n",
    "# Will need to add it once per section\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from typing import List, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI  # Added for OpenAI embeddings\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "class TextSimilarityModel:\n",
    "    def __init__(self, corpus_name, rel_name, model_name='all-MiniLM-L6-v2', top_k=10):\n",
    "        \"\"\"\n",
    "        Initialize the model with datasets and pre-trained sentence transformer.\n",
    "        \"\"\"\n",
    "        load_dotenv()\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            self.model.to('cuda')\n",
    "        else:\n",
    "            print(\"Using CPU\")\n",
    "        \n",
    "        self.corpus_name = corpus_name\n",
    "        self.rel_name = rel_name\n",
    "        self.top_k = top_k\n",
    "        self.session_state = {}\n",
    "        self.load_data()\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and filter datasets based on test queries and documents.\n",
    "        \"\"\"\n",
    "        # Load query and document datasets\n",
    "        dataset_queries = load_dataset(self.corpus_name, \"queries\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "\n",
    "        # Extract queries and documents\n",
    "        self.queries = dataset_queries[\"queries\"][\"text\"]\n",
    "        self.query_ids = dataset_queries[\"queries\"][\"_id\"]\n",
    "        self.documents = dataset_docs[\"corpus\"][\"text\"]\n",
    "        self.document_ids = dataset_docs[\"corpus\"][\"_id\"]\n",
    "\n",
    "                \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on test set\n",
    "        test_qrels = load_dataset(self.rel_name)[\"test\"]\n",
    "        self.filtered_test_query_ids = set(test_qrels[\"query-id\"])\n",
    "        self.filtered_test_doc_ids = set(test_qrels[\"corpus-id\"])\n",
    "\n",
    "        self.test_queries = [q for qid, q in zip(self.query_ids, self.queries) if qid in self.filtered_test_query_ids]\n",
    "        self.test_query_ids = [qid for qid in self.query_ids if qid in self.filtered_test_query_ids]\n",
    "        self.test_documents = [doc for did, doc in zip(self.document_ids, self.documents) if did in self.filtered_test_doc_ids]\n",
    "        self.test_document_ids = [did for did in self.document_ids if did in self.filtered_test_doc_ids]\n",
    "\n",
    "        self.test_query_id_to_relevant_doc_ids = {qid: [] for qid in self.test_query_ids}\n",
    "        for qid, doc_id in zip(test_qrels[\"query-id\"], test_qrels[\"corpus-id\"]):\n",
    "            if qid in self.test_query_id_to_relevant_doc_ids:\n",
    "                self.test_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "                \n",
    "        ## Code Below this is used for creating the training set \n",
    "        # Build query and document id to text mapping\n",
    "        self.query_id_to_text = {query_id:query for query_id, query in zip(self.query_ids, self.queries)}\n",
    "        self.document_id_to_text = {document_id:document for document_id, document in zip(self.document_ids, self.documents)}\n",
    "\n",
    "        # Build relevant queries and documents mapping based on train set\n",
    "        train_qrels = load_dataset(self.rel_name)[\"train\"]\n",
    "        self.train_query_id_to_relevant_doc_ids = {qid: [] for qid in train_qrels[\"query-id\"]}\n",
    "\n",
    "        for qid, doc_id in zip(train_qrels[\"query-id\"], train_qrels[\"corpus-id\"]):\n",
    "            if qid in self.train_query_id_to_relevant_doc_ids:\n",
    "                # Append the document ID to the relevant doc mapping\n",
    "                self.train_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on validation set  \n",
    "        #TODO Put your code here. \n",
    "         ###########################################################################\n",
    "        val_qrels = load_dataset(self.rel_name)[\"validation\"]\n",
    "        self.filtered_val_query_ids = set(val_qrels[\"query-id\"])\n",
    "        self.filtered_val_doc_ids = set(val_qrels[\"corpus-id\"])        \n",
    "\n",
    "        self.val_queries = [q for qid, q in zip(self.query_ids, self.queries) if qid in self.filtered_val_query_ids]\n",
    "        self.val_query_ids = [qid for qid in self.query_ids if qid in self.filtered_val_query_ids]\n",
    "        self.val_documents = [doc for did, doc in zip(self.document_ids, self.documents) if did in self.filtered_val_doc_ids]\n",
    "        self.val_document_ids = [did for did in self.document_ids if did in self.filtered_val_doc_ids]\n",
    "\n",
    "        self.val_query_id_to_relevant_doc_ids = {qid: [] for qid in self.val_query_ids}\n",
    "        for qid, doc_id in zip(val_qrels[\"query-id\"], val_qrels[\"corpus-id\"]):\n",
    "            if qid in self.val_query_id_to_relevant_doc_ids:\n",
    "                self.val_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        ###########################################################################\n",
    "        \n",
    "\n",
    "    #Task 1: Encode Queries and Documents (10 Pts)\n",
    "\n",
    "    def encode_with_glove(self, glove_file_path: str, sentences: list[str]) -> list[np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - glove_file_path (str): Path to the GloVe embeddings file (e.g., \"glove.6B.50d.txt\").\n",
    "            - sentences (list[str]): A list of sentences to encode.\n",
    "\n",
    "        # Output:\n",
    "            - list[np.ndarray]: A list of sentence embeddings \n",
    "            \n",
    "        (1) Encodes sentences by averaging GloVe 50d vectors of words in each sentence.\n",
    "        (2) Return a sequence of embeddings of the sentences.\n",
    "        Download the glove vectors from here. \n",
    "        https://nlp.stanford.edu/data/glove.6B.zip\n",
    "        Handle unknown words by using zero vectors\n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        word_to_vec = {}\n",
    "        embedding_dim = 50\n",
    "    \n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                word_to_vec[word] = vector  \n",
    "        \n",
    "        sentence_embeddings = []\n",
    "        for sentence in tqdm(sentences, desc=\"GloVe encoding\"):\n",
    "            words = sentence.lower().split()\n",
    "            \n",
    "            vectors = []\n",
    "            for word in words:\n",
    "                if word in word_to_vec:\n",
    "                    vectors.append(word_to_vec[word])\n",
    "                else:\n",
    "                    vectors.append(np.zeros(embedding_dim))\n",
    "            \n",
    "            if len(vectors) > 0:\n",
    "                avg_vec = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                avg_vec = np.zeros(embedding_dim)\n",
    "                \n",
    "            sentence_embeddings.append(avg_vec)\n",
    "\n",
    "        return sentence_embeddings\n",
    "        ###########################################################################\n",
    "    \n",
    "\n",
    "    def encode_with_openai(\n",
    "        self,\n",
    "        sentences: List[str], \n",
    "        model: str = 'text-embedding-3-small',\n",
    "        api_key: Optional[str] = None,\n",
    "        batch_size: int = 100\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes sentences using OpenAI's embedding API.\n",
    "        \n",
    "        # Inputs:\n",
    "            - sentences (List[str]): A list of sentences to encode.\n",
    "            - model (str): OpenAI model name. Options:\n",
    "                * 'text-embedding-3-small' (1536 dims, $0.02/1M tokens) - RECOMMENDED\n",
    "                * 'text-embedding-3-large' (3072 dims, $0.13/1M tokens)\n",
    "                * 'text-embedding-ada-002' (1536 dims, legacy)\n",
    "            - api_key (str, optional): OpenAI API key. If None, reads from OPENAI_API_KEY env variable\n",
    "            - batch_size (int): Number of sentences to encode per API call (max 2048)\n",
    "            \n",
    "        Instructions:\n",
    "        - Implement batched encoding with error handling\n",
    "        - Add rate limiting (sleep between batches)\n",
    "        \n",
    "        Expected Cost for this Assignment:\n",
    "        - ~4,000 texts (320 queries + 3,600 documents)\n",
    "        - text-embedding-3-small: ~$0.08-0.10 per student\n",
    "        - text-embedding-3-large: ~$0.50-0.65 per student\n",
    "        \n",
    "        Tips:\n",
    "        - Use try-except for API errors\n",
    "        - Implement retry logic with exponential backoff\n",
    "        - Cache embeddings to avoid re-encoding\n",
    "        - Monitor your usage at: https://platform.openai.com/usage\n",
    "        \"\"\"\n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        cache_key = \"cat_embed_\" + \"openai\" + \"_\" + model\n",
    "        if cache_key not in self.session_state:\n",
    "            self.session_state[cache_key] = {}\n",
    "        if api_key is None:\n",
    "            print(\"Getting key from os\")\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        input = list(set(s for s in sentences if s not in self.session_state[cache_key]))\n",
    "        openai_embedding = []\n",
    "\n",
    "        if not api_key:\n",
    "            raise Exception(\"openai api key not exist\")\n",
    "        \n",
    "        if input:\n",
    "            client = OpenAI(api_key = api_key)\n",
    "            try:\n",
    "                pbar = tqdm(range(0, len(input), batch_size), desc=\"OpenAI API requesting\")\n",
    "                for i in pbar:\n",
    "                    batch = input[i : i + batch_size]\n",
    "                    response = client.embeddings.create(\n",
    "                        input = batch,\n",
    "                        model = model\n",
    "                    )\n",
    "                    results = [data.embedding for data in response.data]\n",
    "                    \n",
    "                    for text, embedding_result in zip(batch, results):\n",
    "                        self.session_state[cache_key][text] = embedding_result\n",
    "                    time.sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting OpenAI embeddings: {e}\")\n",
    "                \n",
    "        dim = 3072 if model == 'text-embedding-3-large' else 1536\n",
    "        \n",
    "        for s in sentences:\n",
    "            if s in self.session_state[cache_key]:\n",
    "                openai_embedding.append(self.session_state[cache_key][s])\n",
    "            else:\n",
    "                openai_embedding.append(np.zeros(dim))\n",
    "                \n",
    "        return np.array(openai_embedding)\n",
    "        ###########################################################################\n",
    "\n",
    "    #Task 2: Calculate Cosine Similarity and Rank Documents (20 Pts)\n",
    "    \n",
    "    def rank_documents(self, encoding_method: str = 'sentence_transformer') -> None:\n",
    "        \"\"\"\n",
    "         # Inputs:\n",
    "            - encoding_method (str): The method used for encoding queries/documents. \n",
    "                             Options: ['glove', 'sentence_transformer'].\n",
    "\n",
    "        # Output:\n",
    "            - None (updates self.query_id_to_ranked_doc_ids with ranked document IDs).\n",
    "    \n",
    "        (1) Compute cosine similarity between each document and the query\n",
    "        (2) Rank documents for each query and save the results in a dictionary \"query_id_to_ranked_doc_ids\" \n",
    "            This will be used in \"mean_average_precision\"\n",
    "            Example format {2: [125, 673], 35: [900, 822]}\n",
    "        \"\"\"\n",
    "        if encoding_method == 'glove':\n",
    "            # Note: Ensure \"glove.6B.50d.txt\" is downloaded and in the local directory\n",
    "            query_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.queries)\n",
    "            document_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "        elif encoding_method == 'sentence_transformer':\n",
    "            query_embeddings = self.model.encode(self.queries, show_progress_bar=True)\n",
    "            document_embeddings = self.model.encode(self.documents, show_progress_bar=True)\n",
    "        elif encoding_method == 'openai':\n",
    "            # Use environment variable or prompt for API key\n",
    "            query_embeddings = self.encode_with_openai(self.queries)\n",
    "            document_embeddings = self.encode_with_openai(self.documents)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid encoding method. Choose 'glove' or 'sentence_transformer'.\")\n",
    "        \n",
    "        \n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "         # define a dictionary to store the ranked documents for each query\n",
    "        self.query_id_to_ranked_doc_ids = {}\n",
    "        similarities = cosine_similarity(query_embeddings, document_embeddings)\n",
    "        print(\"cosine similarity finish\")\n",
    "\n",
    "        query_id_to_idx = {query_id: idx for idx, query_id in enumerate(self.query_ids)}\n",
    "        for query_id in tqdm(self.test_query_ids, desc=f\"Ranking ({encoding_method})\"):\n",
    "            if query_id in query_id_to_idx:\n",
    "                # i query score to all documents\n",
    "                row_idx = query_id_to_idx[query_id]\n",
    "                query_scores = similarities[row_idx]\n",
    "                \n",
    "                # argsort sort ascending，[::-1] to revert\n",
    "                ranked_indices = np.argsort(query_scores)[::-1]\n",
    "                \n",
    "                # track document ID\n",
    "                ranked_doc_ids = [self.document_ids[idx] for idx in ranked_indices]\n",
    "                \n",
    "                # save\n",
    "                self.query_id_to_ranked_doc_ids[query_id] = ranked_doc_ids\n",
    "\n",
    "        print(\"Task 2 Finish\")\n",
    "      \n",
    "        ###########################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def average_precision(relevant_docs: list[str], candidate_docs: list[str]) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - relevant_docs (list[str]): A list of document IDs that are relevant to the query.\n",
    "            - candidate_docs (list[str]): A list of document IDs ranked by the model.\n",
    "\n",
    "        # Output:\n",
    "            - float: The average precision score\n",
    "    \n",
    "        Compute average precision for a single query.\n",
    "        \"\"\"\n",
    "        y_true = [1 if doc_id in relevant_docs else 0 for doc_id in candidate_docs]\n",
    "        precisions = [np.mean(y_true[:k+1]) for k in range(len(y_true)) if y_true[k]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "\n",
    "    #Task 3: Calculate Evaluate System Performance (10 Pts)\n",
    "    \n",
    "    def mean_average_precision(self) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - None (uses ranked documents stored in self.query_id_to_ranked_doc_ids).\n",
    "\n",
    "        # Output:\n",
    "            - float: The MAP score, computed as the mean of all average precision scores.\n",
    "    \n",
    "        (1) Compute mean average precision for all queries using the \"average_precision\" function.\n",
    "        (2) Compute the mean of all average precision scores\n",
    "        Return the mean average precision score\n",
    "        \n",
    "        reference: https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map\n",
    "        https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2\n",
    "        \"\"\"\n",
    "         #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        ap_scores = []\n",
    "        \n",
    "        for query_id in self.test_query_ids:\n",
    "            ranked_docs = self.query_id_to_ranked_doc_ids.get(query_id, [])\n",
    "            \n",
    "            relevant_docs = self.test_query_id_to_relevant_doc_ids.get(query_id, [])\n",
    "            \n",
    "            ap = self.average_precision(relevant_docs, ranked_docs)\n",
    "            ap_scores.append(ap)\n",
    "\n",
    "        if not ap_scores:\n",
    "            return 0.0\n",
    "            \n",
    "        return sum(ap_scores) / len(ap_scores)\n",
    "        ###########################################################################\n",
    "    \n",
    "    #Task 4: Ranking the Top 10 Documents based on Similarity Scores (10 Pts)\n",
    "\n",
    "    def show_ranking_documents(self, encoding_method: str, example_query: str) -> None:\n",
    "                \n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - example_query (str): A query string for which top-ranked documents should be displayed.\n",
    "\n",
    "        # Output:\n",
    "            - None (prints the ranked documents along with similarity scores).\n",
    "        \n",
    "        (1) rank documents with given query with cosine similarity scores\n",
    "        (2) prints the top 10 results along with its similarity score.\n",
    "        \n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "\n",
    "        ###########################################################################\n",
    "      \n",
    "        # 1. Encode the single query based on the method\n",
    "        # 2. Reshape check: Ensure query_embedding is (1, n_features)\n",
    "        # 3. Calculate scores\n",
    "        \n",
    "        if encoding_method == 'glove':\n",
    "            query_embedding = self.encode_with_glove(\"glove.6B.50d.txt\", [example_query])\n",
    "            doc_embs = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "        elif encoding_method == 'sentence_transformer':\n",
    "            query_embedding = self.model.encode([example_query])\n",
    "            doc_embs = self.model.encode(self.documents)\n",
    "        elif encoding_method == 'openai':\n",
    "            query_embedding = self.encode_with_openai([example_query])\n",
    "            doc_embs = self.encode_with_openai(self.documents)\n",
    "        else:\n",
    "            print(\"Invalid encoding method.\")\n",
    "            return\n",
    "\n",
    "        query_embedding = np.array(query_embedding).reshape(1, -1)            \n",
    "\n",
    "        scores = cosine_similarity(query_embedding, doc_embs)[0]\n",
    "        top_10_indices = np.argsort(scores)[::-1][:10]\n",
    "\n",
    "        print(f\"\\n--- Top 10 Results for Query (Method: {encoding_method}) ---\")\n",
    "        print(f\"Query: {example_query}\\n\")\n",
    "\n",
    "        for i, idx in enumerate(top_10_indices):\n",
    "            doc_id = self.document_ids[idx]\n",
    "            score = scores[idx]\n",
    "            text_snippet = self.document_id_to_text.get(doc_id, \"No text available\")[:150]\n",
    "            \n",
    "            print(f\"Rank {i+1} | Score: {score:.4f} | ID: {doc_id}\")\n",
    "            print(f\"Snippet: {text_snippet}...\")\n",
    "            print(\"-\" * 30)\n",
    "        ###########################################################################\n",
    "      \n",
    "    #Task 5:Fine tune the sentence transformer model (25 Pts)\n",
    "    # Students are not graded on achieving a high MAP score. \n",
    "    # The key is to show understanding, experimentation, and thoughtful analysis.\n",
    "    \n",
    "    def fine_tune_model(self, batch_size: int = 32, num_epochs: int = 3, save_model_path: str = \"finetuned_senBERT\") -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Fine-tunes the model using MultipleNegativesRankingLoss.\n",
    "        (1) Prepare training examples from `self.prepare_training_examples()`\n",
    "        (2) Experiment with [anchor, positive] vs [anchor, positive, negative]\n",
    "        (3) Define a loss function (`MultipleNegativesRankingLoss`)\n",
    "        (4) Freeze all model layers except the final layers\n",
    "        (5) Train the model with the specified learning rate\n",
    "        (6) Save the fine-tuned model\n",
    "        \"\"\"\n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        train_examples = self.prepare_training_examples()\n",
    "        if not train_examples:\n",
    "            print(\"Error: No training examples found.\")\n",
    "            return\n",
    "        \n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(model=self.model)\n",
    "        target_model = self.model[0].auto_model\n",
    "\n",
    "        print(\"--- Freezing layers ---\")\n",
    "        for name, param in target_model.named_parameters():\n",
    "            if any(key in name for key in [\"layer.5\", \"layer.4\", \"pooler\"]):\n",
    "                param.requires_grad = True\n",
    "                print(f\"Trainable: {name}\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=int(len(train_dataloader) * 0.1),\n",
    "            output_path=save_model_path,\n",
    "            optimizer_params={'lr': 2e-5},\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        print(f\"Finish: {save_model_path}\")\n",
    "        ###########################################################################\n",
    "\n",
    "    # Take a careful look into how the training set is created\n",
    "    def prepare_training_examples(self) -> list[InputExample]:\n",
    "\n",
    "        \"\"\"\n",
    "        Prepares training examples from the training data.\n",
    "        # Inputs:\n",
    "            - None (uses self.train_query_id_to_relevant_doc_ids to create training pairs).\n",
    "\n",
    "         # Output:\n",
    "            Output: - list[InputExample]: A list of training samples containing [anchor, positive] or [anchor, positive, negative].\n",
    "            \n",
    "        \"\"\"\n",
    "        train_examples = []\n",
    "        all_doc_ids = list(self.document_id_to_text.keys())\n",
    "        for qid, doc_ids in tqdm(self.train_query_id_to_relevant_doc_ids.items(), desc=\"Generating Triplets\"):\n",
    "            relevant_set = set(doc_ids)\n",
    "            for doc_id in doc_ids:\n",
    "                anchor = self.query_id_to_text[qid]\n",
    "                positive = self.document_id_to_text[doc_id]\n",
    "                # TODO: Select random negative examples that are not relevant to the query.\n",
    "                negative_id = random.choice(all_doc_ids)\n",
    "                while negative_id in relevant_set:\n",
    "                    negative_id = random.choice(all_doc_ids)\n",
    "                \n",
    "                negative = self.document_id_to_text[negative_id]\n",
    "                \n",
    "                # TODO: Create list[InputExample] of type [anchor, positive, negative]\n",
    "                train_examples.append(InputExample(texts=[anchor, positive, negative]))\n",
    "                #train_examples.append(InputExample(texts=[anchor, positive]))\n",
    "\n",
    "        return train_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b882ce773fe54b7f8363e90d7ea6dd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Ranking with sentence_transformer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08dd34f5ac444fdaa3d58a971a37c250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b0ac7607214a298cee3a573dfbdf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking (sentence_transformer): 100%|████████████████████████████████████████████████| 323/323 [01:31<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Finish\n",
      "SBERT Mean Average Precision: 0.16038328234687443\n",
      "\n",
      "Ranking with glove...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GloVe encoding: 100%|███████████████████████████████████████████████████████████| 3237/3237 [00:00<00:00, 39056.23it/s]\n",
      "GloVe encoding: 100%|████████████████████████████████████████████████████████████| 3633/3633 [00:00<00:00, 5658.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking (glove): 100%|███████████████████████████████████████████████████████████████| 323/323 [01:50<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Finish\n",
      "GloVe Mean Average Precision: 0.024486486169480685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GloVe encoding: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "GloVe encoding: 100%|████████████████████████████████████████████████████████████| 3633/3633 [00:00<00:00, 4414.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Results for Query (Method: glove) ---\n",
      "Query: Breast Cancer Cells Feed on Cholesterol\n",
      "\n",
      "Rank 1 | Score: 0.8676 | ID: MED-2111\n",
      "Snippet: Coronary artery disease is essentially nonexistent in cultures whose nutrition assures cholesterol levels <150 mg/dl. Patients with advanced coronary ...\n",
      "------------------------------\n",
      "Rank 2 | Score: 0.8423 | ID: MED-5160\n",
      "Snippet: Pine needles (Pinus densiflora Siebold et Zuccarini) have long been used as a traditional health-promoting medicinal food in Korea. To investigate the...\n",
      "------------------------------\n",
      "Rank 3 | Score: 0.8418 | ID: MED-3129\n",
      "Snippet: BRCA1 mutations have been associated with hereditary breast cancer only. Recent studies indicate that a subgroup of sporadic breast cancer might also ...\n",
      "------------------------------\n",
      "Rank 4 | Score: 0.8351 | ID: MED-865\n",
      "Snippet: Prostate cancer remains the second leading cause of cancer deaths among American men. Earlier diagnosis increases survival rate in patients. However, ...\n",
      "------------------------------\n",
      "Rank 5 | Score: 0.8350 | ID: MED-2103\n",
      "Snippet: OBJECTIVE: High concentrations of plasma deoxycholic acid (DCA) are found in human breast cyst fluid and it has been hypothesised that this may be rel...\n",
      "------------------------------\n",
      "Rank 6 | Score: 0.8344 | ID: MED-5155\n",
      "Snippet: Objective: To determine if a supplement of soy protein improves body composition, body fat distribution, and glucose and insulin metabolism in non-dia...\n",
      "------------------------------\n",
      "Rank 7 | Score: 0.8340 | ID: MED-5184\n",
      "Snippet: We examined the association of dietary lignan intake with estrogen receptor negative (ER-) and ER positive (ER+) breast cancer risk in a breast cancer...\n",
      "------------------------------\n",
      "Rank 8 | Score: 0.8334 | ID: MED-5072\n",
      "Snippet: Antioxidant-rich diets are associated with reduced asthma prevalence. However, direct evidence that altering intake of antioxidant-rich foods affects ...\n",
      "------------------------------\n",
      "Rank 9 | Score: 0.8325 | ID: MED-2427\n",
      "Snippet: Lipid rafts/caveolae are membrane platforms for signaling molecules that regulate various cellular functions, including cell survival. To better under...\n",
      "------------------------------\n",
      "Rank 10 | Score: 0.8295 | ID: MED-5304\n",
      "Snippet: PURPOSE OF REVIEW: Brown adipose tissue (BAT), which is present in humans, plays an important role in oxidation of fatty acids and glucose. The purpos...\n",
      "------------------------------\n",
      "\n",
      "--- Top 10 Results for Query (Method: sentence_transformer) ---\n",
      "Query: Breast Cancer Cells Feed on Cholesterol\n",
      "\n",
      "Rank 1 | Score: 0.6946 | ID: MED-2439\n",
      "Snippet: While many factors are involved in the etiology of cancer, it has been clearly established that diet significantly impacts one’s risk for this disease...\n",
      "------------------------------\n",
      "Rank 2 | Score: 0.6723 | ID: MED-2434\n",
      "Snippet: The specific role of dietary fat in breast cancer progression is unclear, although a low-fat diet was associated with decreased recurrence of estrogen...\n",
      "------------------------------\n",
      "Rank 3 | Score: 0.6473 | ID: MED-2440\n",
      "Snippet: Purpose To further clarify the relationship between total cholesterol and cancer, which remains unclear. Methods We prospectively examined the associa...\n",
      "------------------------------\n",
      "Rank 4 | Score: 0.5877 | ID: MED-2427\n",
      "Snippet: Lipid rafts/caveolae are membrane platforms for signaling molecules that regulate various cellular functions, including cell survival. To better under...\n",
      "------------------------------\n",
      "Rank 5 | Score: 0.5498 | ID: MED-2774\n",
      "Snippet: Concern has been expressed about the fact that cows' milk contains estrogens and could stimulate the growth of hormone-sensitive tumors. In this study...\n",
      "------------------------------\n",
      "Rank 6 | Score: 0.5406 | ID: MED-838\n",
      "Snippet: Docosahexaenoic acid (DHA) is an omega-3 fatty acid that comprises 22 carbons and 6 alternative double bonds in its hydrocarbon chain (22:6omega3). Pr...\n",
      "------------------------------\n",
      "Rank 7 | Score: 0.5205 | ID: MED-2430\n",
      "Snippet: The objective of this study was to investigate the effects of the dietary phytosterol beta-sitosterol (SIT) and the antiestrogen drug tamoxifen (TAM) ...\n",
      "------------------------------\n",
      "Rank 8 | Score: 0.5141 | ID: MED-2102\n",
      "Snippet: The effects of the major human serum bile acid, glycochenodeoxycholic acid (GCDC), as well as unconjugated chenodeoxycholic acid (CDC), on the MCF-7 h...\n",
      "------------------------------\n",
      "Rank 9 | Score: 0.5081 | ID: MED-2437\n",
      "Snippet: BACKGROUND: Breast cancer is the most commonly diagnosed cancer among women in the United States. Extensive research has been completed to evaluate th...\n",
      "------------------------------\n",
      "Rank 10 | Score: 0.5012 | ID: MED-5066\n",
      "Snippet: Context Evidence is lacking that a dietary pattern high in vegetables, fruit, and fiber and low in total fat can influence breast cancer recurrence or...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the medical dataset (nfcorpus)\n",
    "model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "\n",
    "# Evaluate using the default Sentence Transformer\n",
    "print(\"Ranking with sentence_transformer...\")\n",
    "model.rank_documents(encoding_method='sentence_transformer')\n",
    "sbert_map = model.mean_average_precision()\n",
    "print(\"SBERT Mean Average Precision:\", sbert_map)\n",
    "\n",
    "# Evaluate using GloVe (requires 'glove.6B.50d.txt' in your directory)\n",
    "print(\"\\nRanking with glove...\")\n",
    "model.rank_documents(encoding_method='glove')\n",
    "glove_map = model.mean_average_precision()\n",
    "print(\"GloVe Mean Average Precision:\", glove_map)\n",
    "\n",
    "# Qualitative test: Show actual document text for a sample query\n",
    "model.show_ranking_documents(\"glove\",\"Breast Cancer Cells Feed on Cholesterol\")\n",
    "model.show_ranking_documents(\"sentence_transformer\", \"Breast Cancer Cells Feed on Cholesterol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abd301ef0ac4152b4540ac10802a140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "Ranking with openai...\n",
      "Getting key from os\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAI API requesting: 100%|███████████████████████████████████████████████████████████| 33/33 [00:41<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting key from os\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAI API requesting: 100%|███████████████████████████████████████████████████████████| 36/36 [00:48<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking (openai): 100%|██████████████████████████████████████████████████████████████| 323/323 [01:45<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Finish\n",
      "openai Mean Average Precision: 0.20002033024968435\n",
      "Getting key from os\n",
      "Getting key from os\n",
      "\n",
      "--- Top 10 Results for Query (Method: openai) ---\n",
      "Query: Breast Cancer Cells Feed on Cholesterol\n",
      "\n",
      "Rank 1 | Score: 0.5738 | ID: MED-2434\n",
      "Snippet: The specific role of dietary fat in breast cancer progression is unclear, although a low-fat diet was associated with decreased recurrence of estrogen...\n",
      "------------------------------\n",
      "Rank 2 | Score: 0.5047 | ID: MED-2439\n",
      "Snippet: While many factors are involved in the etiology of cancer, it has been clearly established that diet significantly impacts one’s risk for this disease...\n",
      "------------------------------\n",
      "Rank 3 | Score: 0.4960 | ID: MED-2427\n",
      "Snippet: Lipid rafts/caveolae are membrane platforms for signaling molecules that regulate various cellular functions, including cell survival. To better under...\n",
      "------------------------------\n",
      "Rank 4 | Score: 0.4907 | ID: MED-3551\n",
      "Snippet: Breast cancer is the leading cause of cancer-related deaths for women in the United States and the rest of the world. About 8% of women develop breast...\n",
      "------------------------------\n",
      "Rank 5 | Score: 0.4840 | ID: MED-2102\n",
      "Snippet: The effects of the major human serum bile acid, glycochenodeoxycholic acid (GCDC), as well as unconjugated chenodeoxycholic acid (CDC), on the MCF-7 h...\n",
      "------------------------------\n",
      "Rank 6 | Score: 0.4762 | ID: MED-2774\n",
      "Snippet: Concern has been expressed about the fact that cows' milk contains estrogens and could stimulate the growth of hormone-sensitive tumors. In this study...\n",
      "------------------------------\n",
      "Rank 7 | Score: 0.4642 | ID: MED-2436\n",
      "Snippet: The content of low density lipoprotein (LDL) receptors in tissue from primary breast cancers was determined and its prognostic information compared wi...\n",
      "------------------------------\n",
      "Rank 8 | Score: 0.4615 | ID: MED-2101\n",
      "Snippet: The notion that a breast-gut connection might modulate the microenvironment of breast tissue was supported by the finding that breast cyst fluid conta...\n",
      "------------------------------\n",
      "Rank 9 | Score: 0.4578 | ID: MED-4117\n",
      "Snippet: Breast cancer is a complex disease. Its aetiology is multifactorial, its period of development can span decades, and its clinical course is highly var...\n",
      "------------------------------\n",
      "Rank 10 | Score: 0.4562 | ID: MED-3781\n",
      "Snippet: In this study, a panel of normal human prostate cells (HPCs) and tumor cells derived from metastases were studied by (1)H NMR spectroscopy to determin...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "print(\"\\nRanking with openai...\")\n",
    "model.rank_documents(encoding_method='openai')\n",
    "openai_map = model.mean_average_precision()\n",
    "print(\"openai Mean Average Precision:\", openai_map)\n",
    "\n",
    "model.show_ranking_documents(\"openai\",\"Breast Cancer Cells Feed on Cholesterol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621e84d985f549f7a8e5348eaf025999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Triplets: 100%|███████████████████████████████████████████████████████| 2590/2590 [00:00<00:00, 4505.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Freezing layers ---\n",
      "Trainable: encoder.layer.4.attention.self.query.weight\n",
      "Trainable: encoder.layer.4.attention.self.query.bias\n",
      "Trainable: encoder.layer.4.attention.self.key.weight\n",
      "Trainable: encoder.layer.4.attention.self.key.bias\n",
      "Trainable: encoder.layer.4.attention.self.value.weight\n",
      "Trainable: encoder.layer.4.attention.self.value.bias\n",
      "Trainable: encoder.layer.4.attention.output.dense.weight\n",
      "Trainable: encoder.layer.4.attention.output.dense.bias\n",
      "Trainable: encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Trainable: encoder.layer.4.intermediate.dense.weight\n",
      "Trainable: encoder.layer.4.intermediate.dense.bias\n",
      "Trainable: encoder.layer.4.output.dense.weight\n",
      "Trainable: encoder.layer.4.output.dense.bias\n",
      "Trainable: encoder.layer.4.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.4.output.LayerNorm.bias\n",
      "Trainable: encoder.layer.5.attention.self.query.weight\n",
      "Trainable: encoder.layer.5.attention.self.query.bias\n",
      "Trainable: encoder.layer.5.attention.self.key.weight\n",
      "Trainable: encoder.layer.5.attention.self.key.bias\n",
      "Trainable: encoder.layer.5.attention.self.value.weight\n",
      "Trainable: encoder.layer.5.attention.self.value.bias\n",
      "Trainable: encoder.layer.5.attention.output.dense.weight\n",
      "Trainable: encoder.layer.5.attention.output.dense.bias\n",
      "Trainable: encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Trainable: encoder.layer.5.intermediate.dense.weight\n",
      "Trainable: encoder.layer.5.intermediate.dense.bias\n",
      "Trainable: encoder.layer.5.output.dense.weight\n",
      "Trainable: encoder.layer.5.output.dense.bias\n",
      "Trainable: encoder.layer.5.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.5.output.LayerNorm.bias\n",
      "Trainable: pooler.dense.weight\n",
      "Trainable: pooler.dense.bias\n",
      "Starting training for 2 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138704b4286e4b6580c3018e952da205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73718' max='73718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [73718/73718 1:02:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.878652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.806926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.630516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.539556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.530074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.472565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.488967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.474710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.436949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.452277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.424466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.394822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.442525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.427268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.420545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.412246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.364819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.377347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.350885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.357121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.357405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.308214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.336508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.326046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.337244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.333951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.348502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.303816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.335421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.332546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.310937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.303483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.277360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.265886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.306572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.249684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.267826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.255868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.247571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.280023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.241037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.317788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.245514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.199052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.296830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.253539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.258768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.260464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.283353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.261934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.223047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.209786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.205469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.242705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.221495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.211627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>1.203852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.216443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>1.189732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.212161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>1.244171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.225029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>1.161524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.229064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.175715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.172316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>1.206650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.209373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>1.200068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.204293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>1.166366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.223723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>1.180153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>1.154973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>1.108444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>1.054012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>1.080534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>1.089340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>1.130426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.094030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>1.123786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>1.063438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>1.090904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>1.133092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.108653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>1.087686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>1.094427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>1.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>1.122009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.073915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>1.085234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>1.089337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>1.076332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>1.088507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>1.048989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>1.031499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>1.131179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>1.068713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>1.062602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.100859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>1.136404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>1.029403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>1.102986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>1.056264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>1.056364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.097389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>1.062064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>1.095122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>1.044644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.093316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>1.089868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>1.055520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>1.063403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>1.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>1.014772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>1.034969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>1.054521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>1.034973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>1.082588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.032474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>1.064326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>1.075145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>1.057346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>1.089194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>1.006622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>1.115973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>1.050469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>1.075323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>1.033529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.001871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>1.065286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>1.070275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>1.039859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>1.050508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>1.035285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>1.078178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>1.007907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>1.010370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>1.064794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.059291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>1.026552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>1.066689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>1.004990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>1.027312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>1.134095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>1.016141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>1.090495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9372696560f42f8a9155bf7c02f8cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: finetuned_senBERT_train_v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d159c2c3ec0c420399b336cdb4d8f289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ede0033e1234d9b985c9bf6add2d9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking (sentence_transformer): 100%|████████████████████████████████████████████████| 323/323 [01:50<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Finish\n",
      "Mean Average Precision: 0.20584097819372285\n"
     ]
    }
   ],
   "source": [
    "# Finetune all-MiniLM-L6-v2 sentence transformer model\n",
    "model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "model.fine_tune_model(batch_size=3, num_epochs=2, save_model_path=\"finetuned_senBERT_train_v2\")  # Adjust batch size and epochs as needed\n",
    "\n",
    "model.rank_documents()\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarityModel_fine_tune_positive:\n",
    "    def __init__(self, corpus_name, rel_name, model_name='all-MiniLM-L6-v2', top_k=10):\n",
    "        \"\"\"\n",
    "        Initialize the model with datasets and pre-trained sentence transformer.\n",
    "        \"\"\"\n",
    "        load_dotenv()\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            self.model.to('cuda')\n",
    "        else:\n",
    "            print(\"Using CPU\")\n",
    "        \n",
    "        self.corpus_name = corpus_name\n",
    "        self.rel_name = rel_name\n",
    "        self.top_k = top_k\n",
    "        self.session_state = {}\n",
    "        self.load_data()\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and filter datasets based on test queries and documents.\n",
    "        \"\"\"\n",
    "        # Load query and document datasets\n",
    "        dataset_queries = load_dataset(self.corpus_name, \"queries\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "\n",
    "        # Extract queries and documents\n",
    "        self.queries = dataset_queries[\"queries\"][\"text\"]\n",
    "        self.query_ids = dataset_queries[\"queries\"][\"_id\"]\n",
    "        self.documents = dataset_docs[\"corpus\"][\"text\"]\n",
    "        self.document_ids = dataset_docs[\"corpus\"][\"_id\"]\n",
    "\n",
    "                \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on test set\n",
    "        test_qrels = load_dataset(self.rel_name)[\"test\"]\n",
    "        self.filtered_test_query_ids = set(test_qrels[\"query-id\"])\n",
    "        self.filtered_test_doc_ids = set(test_qrels[\"corpus-id\"])\n",
    "\n",
    "        self.test_queries = [q for qid, q in zip(self.query_ids, self.queries) if qid in self.filtered_test_query_ids]\n",
    "        self.test_query_ids = [qid for qid in self.query_ids if qid in self.filtered_test_query_ids]\n",
    "        self.test_documents = [doc for did, doc in zip(self.document_ids, self.documents) if did in self.filtered_test_doc_ids]\n",
    "        self.test_document_ids = [did for did in self.document_ids if did in self.filtered_test_doc_ids]\n",
    "\n",
    "        self.test_query_id_to_relevant_doc_ids = {qid: [] for qid in self.test_query_ids}\n",
    "        for qid, doc_id in zip(test_qrels[\"query-id\"], test_qrels[\"corpus-id\"]):\n",
    "            if qid in self.test_query_id_to_relevant_doc_ids:\n",
    "                self.test_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "                \n",
    "        ## Code Below this is used for creating the training set \n",
    "        # Build query and document id to text mapping\n",
    "        self.query_id_to_text = {query_id:query for query_id, query in zip(self.query_ids, self.queries)}\n",
    "        self.document_id_to_text = {document_id:document for document_id, document in zip(self.document_ids, self.documents)}\n",
    "\n",
    "        # Build relevant queries and documents mapping based on train set\n",
    "        train_qrels = load_dataset(self.rel_name)[\"train\"]\n",
    "        self.train_query_id_to_relevant_doc_ids = {qid: [] for qid in train_qrels[\"query-id\"]}\n",
    "\n",
    "        for qid, doc_id in zip(train_qrels[\"query-id\"], train_qrels[\"corpus-id\"]):\n",
    "            if qid in self.train_query_id_to_relevant_doc_ids:\n",
    "                # Append the document ID to the relevant doc mapping\n",
    "                self.train_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on validation set  \n",
    "        #TODO Put your code here. \n",
    "         ###########################################################################\n",
    "        val_qrels = load_dataset(self.rel_name)[\"validation\"]\n",
    "        self.filtered_val_query_ids = set(val_qrels[\"query-id\"])\n",
    "        self.filtered_val_doc_ids = set(val_qrels[\"corpus-id\"])        \n",
    "\n",
    "        self.val_queries = [q for qid, q in zip(self.query_ids, self.queries) if qid in self.filtered_val_query_ids]\n",
    "        self.val_query_ids = [qid for qid in self.query_ids if qid in self.filtered_val_query_ids]\n",
    "        self.val_documents = [doc for did, doc in zip(self.document_ids, self.documents) if did in self.filtered_val_doc_ids]\n",
    "        self.val_document_ids = [did for did in self.document_ids if did in self.filtered_val_doc_ids]\n",
    "\n",
    "        self.val_query_id_to_relevant_doc_ids = {qid: [] for qid in self.val_query_ids}\n",
    "        for qid, doc_id in zip(val_qrels[\"query-id\"], val_qrels[\"corpus-id\"]):\n",
    "            if qid in self.val_query_id_to_relevant_doc_ids:\n",
    "                self.val_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        ###########################################################################\n",
    "        \n",
    "\n",
    "    #Task 1: Encode Queries and Documents (10 Pts)\n",
    "\n",
    "    def encode_with_glove(self, glove_file_path: str, sentences: list[str]) -> list[np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - glove_file_path (str): Path to the GloVe embeddings file (e.g., \"glove.6B.50d.txt\").\n",
    "            - sentences (list[str]): A list of sentences to encode.\n",
    "\n",
    "        # Output:\n",
    "            - list[np.ndarray]: A list of sentence embeddings \n",
    "            \n",
    "        (1) Encodes sentences by averaging GloVe 50d vectors of words in each sentence.\n",
    "        (2) Return a sequence of embeddings of the sentences.\n",
    "        Download the glove vectors from here. \n",
    "        https://nlp.stanford.edu/data/glove.6B.zip\n",
    "        Handle unknown words by using zero vectors\n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        word_to_vec = {}\n",
    "        embedding_dim = 50\n",
    "    \n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                word_to_vec[word] = vector  \n",
    "        \n",
    "        sentence_embeddings = []\n",
    "        for sentence in tqdm(sentences, desc=\"GloVe encoding\"):\n",
    "            words = sentence.lower().split()\n",
    "            \n",
    "            vectors = []\n",
    "            for word in words:\n",
    "                if word in word_to_vec:\n",
    "                    vectors.append(word_to_vec[word])\n",
    "                else:\n",
    "                    vectors.append(np.zeros(embedding_dim))\n",
    "            \n",
    "            if len(vectors) > 0:\n",
    "                avg_vec = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                avg_vec = np.zeros(embedding_dim)\n",
    "                \n",
    "            sentence_embeddings.append(avg_vec)\n",
    "\n",
    "        return sentence_embeddings\n",
    "        ###########################################################################\n",
    "    \n",
    "\n",
    "    def encode_with_openai(\n",
    "        self,\n",
    "        sentences: List[str], \n",
    "        model: str = 'text-embedding-3-small',\n",
    "        api_key: Optional[str] = None,\n",
    "        batch_size: int = 100\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes sentences using OpenAI's embedding API.\n",
    "        \n",
    "        # Inputs:\n",
    "            - sentences (List[str]): A list of sentences to encode.\n",
    "            - model (str): OpenAI model name. Options:\n",
    "                * 'text-embedding-3-small' (1536 dims, $0.02/1M tokens) - RECOMMENDED\n",
    "                * 'text-embedding-3-large' (3072 dims, $0.13/1M tokens)\n",
    "                * 'text-embedding-ada-002' (1536 dims, legacy)\n",
    "            - api_key (str, optional): OpenAI API key. If None, reads from OPENAI_API_KEY env variable\n",
    "            - batch_size (int): Number of sentences to encode per API call (max 2048)\n",
    "            \n",
    "        Instructions:\n",
    "        - Implement batched encoding with error handling\n",
    "        - Add rate limiting (sleep between batches)\n",
    "        \n",
    "        Expected Cost for this Assignment:\n",
    "        - ~4,000 texts (320 queries + 3,600 documents)\n",
    "        - text-embedding-3-small: ~$0.08-0.10 per student\n",
    "        - text-embedding-3-large: ~$0.50-0.65 per student\n",
    "        \n",
    "        Tips:\n",
    "        - Use try-except for API errors\n",
    "        - Implement retry logic with exponential backoff\n",
    "        - Cache embeddings to avoid re-encoding\n",
    "        - Monitor your usage at: https://platform.openai.com/usage\n",
    "        \"\"\"\n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        cache_key = \"cat_embed_\" + \"openai\" + \"_\" + model\n",
    "        if cache_key not in self.session_state:\n",
    "            self.session_state[cache_key] = {}\n",
    "        if api_key is None:\n",
    "            print(\"Getting key from os\")\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        input = list(set(s for s in sentences if s not in self.session_state[cache_key]))\n",
    "        openai_embedding = []\n",
    "\n",
    "        if not api_key:\n",
    "            raise Exception(\"openai api key not exist\")\n",
    "        \n",
    "        if input:\n",
    "            client = OpenAI(api_key = api_key)\n",
    "            try:\n",
    "                pbar = tqdm(range(0, len(input), batch_size), desc=\"OpenAI API requesting\")\n",
    "                for i in pbar:\n",
    "                    batch = input[i : i + batch_size]\n",
    "                    response = client.embeddings.create(\n",
    "                        input = batch,\n",
    "                        model = model\n",
    "                    )\n",
    "                    results = [data.embedding for data in response.data]\n",
    "                    \n",
    "                    for text, embedding_result in zip(batch, results):\n",
    "                        self.session_state[cache_key][text] = embedding_result\n",
    "                    time.sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting OpenAI embeddings: {e}\")\n",
    "                \n",
    "        dim = 3072 if model == 'text-embedding-3-large' else 1536\n",
    "        \n",
    "        for s in sentences:\n",
    "            if s in self.session_state[cache_key]:\n",
    "                openai_embedding.append(self.session_state[cache_key][s])\n",
    "            else:\n",
    "                openai_embedding.append(np.zeros(dim))\n",
    "                \n",
    "        return np.array(openai_embedding)\n",
    "        ###########################################################################\n",
    "\n",
    "    #Task 2: Calculate Cosine Similarity and Rank Documents (20 Pts)\n",
    "    \n",
    "    def rank_documents(self, encoding_method: str = 'sentence_transformer') -> None:\n",
    "        \"\"\"\n",
    "         # Inputs:\n",
    "            - encoding_method (str): The method used for encoding queries/documents. \n",
    "                             Options: ['glove', 'sentence_transformer'].\n",
    "\n",
    "        # Output:\n",
    "            - None (updates self.query_id_to_ranked_doc_ids with ranked document IDs).\n",
    "    \n",
    "        (1) Compute cosine similarity between each document and the query\n",
    "        (2) Rank documents for each query and save the results in a dictionary \"query_id_to_ranked_doc_ids\" \n",
    "            This will be used in \"mean_average_precision\"\n",
    "            Example format {2: [125, 673], 35: [900, 822]}\n",
    "        \"\"\"\n",
    "        if encoding_method == 'glove':\n",
    "            # Note: Ensure \"glove.6B.50d.txt\" is downloaded and in the local directory\n",
    "            query_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.queries)\n",
    "            document_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "        elif encoding_method == 'sentence_transformer':\n",
    "            query_embeddings = self.model.encode(self.queries, show_progress_bar=True)\n",
    "            document_embeddings = self.model.encode(self.documents, show_progress_bar=True)\n",
    "        elif encoding_method == 'openai':\n",
    "            # Use environment variable or prompt for API key\n",
    "            query_embeddings = self.encode_with_openai(self.queries)\n",
    "            document_embeddings = self.encode_with_openai(self.documents)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid encoding method. Choose 'glove' or 'sentence_transformer'.\")\n",
    "        \n",
    "        \n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "         # define a dictionary to store the ranked documents for each query\n",
    "        self.query_id_to_ranked_doc_ids = {}\n",
    "        similarities = cosine_similarity(query_embeddings, document_embeddings)\n",
    "        print(\"cosine similarity finish\")\n",
    "\n",
    "        query_id_to_idx = {query_id: idx for idx, query_id in enumerate(self.query_ids)}\n",
    "        for query_id in tqdm(self.test_query_ids, desc=f\"Ranking ({encoding_method})\"):\n",
    "            if query_id in query_id_to_idx:\n",
    "                # i query score to all documents\n",
    "                row_idx = query_id_to_idx[query_id]\n",
    "                query_scores = similarities[row_idx]\n",
    "                \n",
    "                # argsort sort ascending，[::-1] to revert\n",
    "                ranked_indices = np.argsort(query_scores)[::-1]\n",
    "                \n",
    "                # track document ID\n",
    "                ranked_doc_ids = [self.document_ids[idx] for idx in ranked_indices]\n",
    "                \n",
    "                # save\n",
    "                self.query_id_to_ranked_doc_ids[query_id] = ranked_doc_ids\n",
    "\n",
    "        print(\"Task 2 Finish\")\n",
    "      \n",
    "        ###########################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def average_precision(relevant_docs: list[str], candidate_docs: list[str]) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - relevant_docs (list[str]): A list of document IDs that are relevant to the query.\n",
    "            - candidate_docs (list[str]): A list of document IDs ranked by the model.\n",
    "\n",
    "        # Output:\n",
    "            - float: The average precision score\n",
    "    \n",
    "        Compute average precision for a single query.\n",
    "        \"\"\"\n",
    "        y_true = [1 if doc_id in relevant_docs else 0 for doc_id in candidate_docs]\n",
    "        precisions = [np.mean(y_true[:k+1]) for k in range(len(y_true)) if y_true[k]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "\n",
    "    #Task 3: Calculate Evaluate System Performance (10 Pts)\n",
    "    \n",
    "    def mean_average_precision(self) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - None (uses ranked documents stored in self.query_id_to_ranked_doc_ids).\n",
    "\n",
    "        # Output:\n",
    "            - float: The MAP score, computed as the mean of all average precision scores.\n",
    "    \n",
    "        (1) Compute mean average precision for all queries using the \"average_precision\" function.\n",
    "        (2) Compute the mean of all average precision scores\n",
    "        Return the mean average precision score\n",
    "        \n",
    "        reference: https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map\n",
    "        https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2\n",
    "        \"\"\"\n",
    "         #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        ap_scores = []\n",
    "        \n",
    "        for query_id in self.test_query_ids:\n",
    "            ranked_docs = self.query_id_to_ranked_doc_ids.get(query_id, [])\n",
    "            \n",
    "            relevant_docs = self.test_query_id_to_relevant_doc_ids.get(query_id, [])\n",
    "            \n",
    "            ap = self.average_precision(relevant_docs, ranked_docs)\n",
    "            ap_scores.append(ap)\n",
    "\n",
    "        if not ap_scores:\n",
    "            return 0.0\n",
    "            \n",
    "        return sum(ap_scores) / len(ap_scores)\n",
    "        ###########################################################################\n",
    "    \n",
    "    #Task 4: Ranking the Top 10 Documents based on Similarity Scores (10 Pts)\n",
    "\n",
    "    def show_ranking_documents(self, encoding_method: str, example_query: str) -> None:\n",
    "                \n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - example_query (str): A query string for which top-ranked documents should be displayed.\n",
    "\n",
    "        # Output:\n",
    "            - None (prints the ranked documents along with similarity scores).\n",
    "        \n",
    "        (1) rank documents with given query with cosine similarity scores\n",
    "        (2) prints the top 10 results along with its similarity score.\n",
    "        \n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "\n",
    "        ###########################################################################\n",
    "      \n",
    "        # 1. Encode the single query based on the method\n",
    "        # 2. Reshape check: Ensure query_embedding is (1, n_features)\n",
    "        # 3. Calculate scores\n",
    "        \n",
    "        if encoding_method == 'glove':\n",
    "            query_embedding = self.encode_with_glove(\"glove.6B.50d.txt\", [example_query])\n",
    "            doc_embs = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "        elif encoding_method == 'sentence_transformer':\n",
    "            query_embedding = self.model.encode([example_query])\n",
    "            doc_embs = self.model.encode(self.documents)\n",
    "        elif encoding_method == 'openai':\n",
    "            query_embedding = self.encode_with_openai([example_query])\n",
    "            doc_embs = self.encode_with_openai(self.documents)\n",
    "        else:\n",
    "            print(\"Invalid encoding method.\")\n",
    "            return\n",
    "\n",
    "        query_embedding = np.array(query_embedding).reshape(1, -1)            \n",
    "\n",
    "        scores = cosine_similarity(query_embedding, doc_embs)[0]\n",
    "        top_10_indices = np.argsort(scores)[::-1][:10]\n",
    "\n",
    "        print(f\"\\n--- Top 10 Results for Query (Method: {encoding_method}) ---\")\n",
    "        print(f\"Query: {example_query}\\n\")\n",
    "\n",
    "        for i, idx in enumerate(top_10_indices):\n",
    "            doc_id = self.document_ids[idx]\n",
    "            score = scores[idx]\n",
    "            text_snippet = self.document_id_to_text.get(doc_id, \"No text available\")[:150]\n",
    "            \n",
    "            print(f\"Rank {i+1} | Score: {score:.4f} | ID: {doc_id}\")\n",
    "            print(f\"Snippet: {text_snippet}...\")\n",
    "            print(\"-\" * 30)\n",
    "        ###########################################################################\n",
    "      \n",
    "    #Task 5:Fine tune the sentence transformer model (25 Pts)\n",
    "    # Students are not graded on achieving a high MAP score. \n",
    "    # The key is to show understanding, experimentation, and thoughtful analysis.\n",
    "    \n",
    "    def fine_tune_model(self, batch_size: int = 32, num_epochs: int = 3, save_model_path: str = \"finetuned_senBERT\") -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Fine-tunes the model using MultipleNegativesRankingLoss.\n",
    "        (1) Prepare training examples from `self.prepare_training_examples()`\n",
    "        (2) Experiment with [anchor, positive] vs [anchor, positive, negative]\n",
    "        (3) Define a loss function (`MultipleNegativesRankingLoss`)\n",
    "        (4) Freeze all model layers except the final layers\n",
    "        (5) Train the model with the specified learning rate\n",
    "        (6) Save the fine-tuned model\n",
    "        \"\"\"\n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        train_examples = self.prepare_training_examples()\n",
    "        if not train_examples:\n",
    "            print(\"Error: No training examples found.\")\n",
    "            return\n",
    "        \n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(model=self.model)\n",
    "        target_model = self.model[0].auto_model\n",
    "\n",
    "        print(\"--- Freezing layers ---\")\n",
    "        for name, param in target_model.named_parameters():\n",
    "            if any(key in name for key in [\"layer.5\", \"layer.4\", \"pooler\"]):\n",
    "                param.requires_grad = True\n",
    "                print(f\"Trainable: {name}\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=int(len(train_dataloader) * 0.1),\n",
    "            output_path=save_model_path,\n",
    "            optimizer_params={'lr': 2e-5},\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        print(f\"Finish: {save_model_path}\")\n",
    "        ###########################################################################\n",
    "\n",
    "    # Take a careful look into how the training set is created\n",
    "    def prepare_training_examples(self) -> list[InputExample]:\n",
    "\n",
    "        \"\"\"\n",
    "        Prepares training examples from the training data.\n",
    "        # Inputs:\n",
    "            - None (uses self.train_query_id_to_relevant_doc_ids to create training pairs).\n",
    "\n",
    "         # Output:\n",
    "            Output: - list[InputExample]: A list of training samples containing [anchor, positive] or [anchor, positive, negative].\n",
    "            \n",
    "        \"\"\"\n",
    "        train_examples = []\n",
    "        all_doc_ids = list(self.document_id_to_text.keys())\n",
    "        for qid, doc_ids in tqdm(self.train_query_id_to_relevant_doc_ids.items(), desc=\"Generating Triplets\"):\n",
    "            relevant_set = set(doc_ids)\n",
    "            for doc_id in doc_ids:\n",
    "                anchor = self.query_id_to_text[qid]\n",
    "                positive = self.document_id_to_text[doc_id]\n",
    "                # TODO: Select random negative examples that are not relevant to the query.\n",
    "                # negative_id = random.choice(all_doc_ids)\n",
    "                # while negative_id in relevant_set:\n",
    "                #     negative_id = random.choice(all_doc_ids)\n",
    "                \n",
    "                # negative = self.document_id_to_text[negative_id]\n",
    "                \n",
    "                # TODO: Create list[InputExample] of type [anchor, positive, negative]\n",
    "                #train_examples.append(InputExample(texts=[anchor, positive, negative]))\n",
    "                train_examples.append(InputExample(texts=[anchor, positive]))\n",
    "\n",
    "        return train_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026b9aae0aea4d91a8013a9e996425e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Triplets: 100%|███████████████████████████████████████████████████████| 2590/2590 [00:00<00:00, 3895.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Freezing layers ---\n",
      "Trainable: encoder.layer.4.attention.self.query.weight\n",
      "Trainable: encoder.layer.4.attention.self.query.bias\n",
      "Trainable: encoder.layer.4.attention.self.key.weight\n",
      "Trainable: encoder.layer.4.attention.self.key.bias\n",
      "Trainable: encoder.layer.4.attention.self.value.weight\n",
      "Trainable: encoder.layer.4.attention.self.value.bias\n",
      "Trainable: encoder.layer.4.attention.output.dense.weight\n",
      "Trainable: encoder.layer.4.attention.output.dense.bias\n",
      "Trainable: encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Trainable: encoder.layer.4.intermediate.dense.weight\n",
      "Trainable: encoder.layer.4.intermediate.dense.bias\n",
      "Trainable: encoder.layer.4.output.dense.weight\n",
      "Trainable: encoder.layer.4.output.dense.bias\n",
      "Trainable: encoder.layer.4.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.4.output.LayerNorm.bias\n",
      "Trainable: encoder.layer.5.attention.self.query.weight\n",
      "Trainable: encoder.layer.5.attention.self.query.bias\n",
      "Trainable: encoder.layer.5.attention.self.key.weight\n",
      "Trainable: encoder.layer.5.attention.self.key.bias\n",
      "Trainable: encoder.layer.5.attention.self.value.weight\n",
      "Trainable: encoder.layer.5.attention.self.value.bias\n",
      "Trainable: encoder.layer.5.attention.output.dense.weight\n",
      "Trainable: encoder.layer.5.attention.output.dense.bias\n",
      "Trainable: encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Trainable: encoder.layer.5.intermediate.dense.weight\n",
      "Trainable: encoder.layer.5.intermediate.dense.bias\n",
      "Trainable: encoder.layer.5.output.dense.weight\n",
      "Trainable: encoder.layer.5.output.dense.bias\n",
      "Trainable: encoder.layer.5.output.LayerNorm.weight\n",
      "Trainable: encoder.layer.5.output.LayerNorm.bias\n",
      "Trainable: pooler.dense.weight\n",
      "Trainable: pooler.dense.bias\n",
      "Starting training for 2 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77a6125bf5b456ba9113ae26679d14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73718' max='73718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [73718/73718 43:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.228467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.195608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.025543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.975159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.946662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.917097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.933878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.916814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.863546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.887330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.879046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.881425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.861918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.856622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.875781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.868150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.900706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.871250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.850830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.866911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.853692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.837314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.837560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.846427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.829118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.812489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.852173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.845790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.799530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.784600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.814975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.825726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.824696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.838443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.810839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.827053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.772811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.799801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.807247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.738705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.753121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.818750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.785066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.803775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.766784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.807669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.769493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.786918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.775651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.761902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.810659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.785218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.806726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.757008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.725053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.814788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.789622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.736222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.751655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.769904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.789618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.777163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.762887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.770744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.754993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.759285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.739854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.755244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.763855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.717066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.761622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.757983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.734553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.703260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.697651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.717882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.710038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.737361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.710649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.719708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.706799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.717722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.713088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.705969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.701841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.751436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.748533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.675766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.686647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.676773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.683431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.709514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.739643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.687435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.697296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.741339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.707122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.693263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.679875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.717203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.680147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.735268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.700302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.696341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.719779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.684490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.677608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.636138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.694431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.666014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.663838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.719096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.746719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.664579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.665256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.685542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.677214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.644070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.663346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.674338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.721740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.650320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.690262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.696541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.669360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.690104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.680357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.669395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.680198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.678071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.659032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.695983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.691669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.724602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.679931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.658043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.686195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.647688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.706140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.665659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.717361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.658016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.653987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.735313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.714672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.691273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d42976eeb6043d092741d1e666e831b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: finetuned_senBERT_train_v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6d72e2b5b04918950ee100dd106390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1926cb962ed64d3e826c17cc5f982f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking (sentence_transformer): 100%|████████████████████████████████████████████████| 323/323 [01:44<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Finish\n",
      "Mean Average Precision: 0.19238795085269614\n"
     ]
    }
   ],
   "source": [
    "# Finetune all-MiniLM-L6-v2 sentence transformer model\n",
    "model_fine_tune_positive = TextSimilarityModel_fine_tune_positive(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "model_fine_tune_positive.fine_tune_model(batch_size=3, num_epochs=2, save_model_path=\"finetuned_senBERT_train_v2\")  # Adjust batch size and epochs as needed\n",
    "\n",
    "model_fine_tune_positive.rank_documents()\n",
    "map_score = model_fine_tune_positive.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_GPU)",
   "language": "python",
   "name": "ml_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
